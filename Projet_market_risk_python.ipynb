{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/llaume974/Market-risk-measure-with-non-parametric-VaR/blob/main/Projet_market_risk_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part A"
      ],
      "metadata": {
        "id": "Ehr-7qVMp07M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importation"
      ],
      "metadata": {
        "id": "i62qZasRqd0W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AKZyvmnpOqZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pywt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "file_path = '/content/Natixis stock (dataset TD1&2).txt'\n",
        "natixis_data = pd.read_csv(file_path, delimiter='\\t')\n",
        "natixis_data.columns = ['Date', 'Price']\n",
        "\n",
        "#convert 'Date' column to datetime format with the correct format\n",
        "natixis_data['Date'] = pd.to_datetime(natixis_data['Date'], format='%d/%m/%Y')\n",
        "\n",
        "#replace commas with periods in the 'Price' column\n",
        "natixis_data['Price'] = natixis_data['Price'].str.replace(',', '.')\n",
        "\n",
        "#convert 'Price' column to a numeric type\n",
        "natixis_data['Price'] = pd.to_numeric(natixis_data['Price'], errors='coerce')\n",
        "\n",
        "#calculate daily returns\n",
        "natixis_data['Return'] = natixis_data['Price'].pct_change()\n",
        "\n",
        "#split data in gains and loses\n",
        "gains = natixis_data['Return'][natixis_data['Return'] > 0]\n",
        "losses = -natixis_data['Return'][natixis_data['Return'] < 0]\n",
        "\n",
        "#filter the dataset for the specified periods\n",
        "data_filtered1 = natixis_data[(natixis_data['Date'] >= '2015-01-01') & (natixis_data['Date'] <= '2016-12-31')]\n",
        "data_filtered2 = natixis_data[(natixis_data['Date'] >= '2017-01-01') & (natixis_data['Date'] <= '2018-12-31')]\n"
      ],
      "metadata": {
        "id": "tF_a_dCvsW9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**A.**"
      ],
      "metadata": {
        "id": "Dwc9iMkhqMAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the time series of the daily prices of the stock Natixis between January 2015 and December 2016, provided with TD1, estimate a historical VaR on price returns at a one-day horizon for a given probability level (this probability is a parameter which must be changed easily). You must base your VaR on a non-parametric distribution\n"
      ],
      "metadata": {
        "id": "NdhvWpExqcEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define functions"
      ],
      "metadata": {
        "id": "Hp9h5Fgoq5tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def empirical_var(data, alpha, bandwidth):\n",
        "    # Clean the data by removing NaNs and infinities\n",
        "    cleaned_data = data[~np.isnan(data[\"Return\"]) & ~np.isinf(data[\"Return\"])]\n",
        "\n",
        "    # Extract the returns after cleaning\n",
        "    returns = cleaned_data[\"Return\"].values\n",
        "    n = len(returns)\n",
        "\n",
        "    # Create an array of return values over which to evaluate the KDE\n",
        "    return_values = np.linspace(returns.min(), returns.max(), 1000)\n",
        "\n",
        "    # Calculate the kernel density estimate using the biweight kernel\n",
        "    kde_values = np.zeros_like(return_values)\n",
        "    for i in range(len(return_values)):\n",
        "        u = (return_values[i] - returns) / bandwidth\n",
        "        kernel_values = (15/16) * (1 - u**2)**2 * (np.abs(u) <= 1)\n",
        "        kde_values[i] = kernel_values.sum() / (bandwidth * n)\n",
        "\n",
        "    # Compute the cumulative distribution function (CDF) from the KDE values\n",
        "    cdf_values = np.cumsum(kde_values) / np.sum(kde_values)\n",
        "\n",
        "    # Interpolate to find the VaR corresponding to the given alpha level\n",
        "    var = np.interp(1-alpha, cdf_values, return_values)\n",
        "\n",
        "    return var"
      ],
      "metadata": {
        "id": "3lbmiBoy3GVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###VaR calculation"
      ],
      "metadata": {
        "id": "yzzBRKgUwHQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scott empirical bandwidth\n",
        "bandwidth = (4 / (3 * len(data_filtered1[\"Return\"]))) ** 0.2 * np.std(data_filtered1[\"Return\"])\n",
        "proba = 0.95\n",
        "\n",
        "\n",
        "var = empirical_var(data_filtered1, proba, bandwidth)\n",
        "print(\"Historical VaR computed with biweight kernel function:\", var)"
      ],
      "metadata": {
        "id": "0RQJM2tepxHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39b38a5e-a001-4ad0-9f66-58b67eb04972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Historical VaR computed with biweight kernel function: -0.03793175395568973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**B.**"
      ],
      "metadata": {
        "id": "uWlpn2YxvwOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which proportion of price returns between January 2017 and December 2018 exceed the VaR\n",
        "threshold defined in the previous question? Do you validate the choice of this non-parametric VaR?"
      ],
      "metadata": {
        "id": "xiZgssfSxicc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#count the number of returns that exceed the VaR threshold\n",
        "exceedances = (data_filtered2['Return'] < var).sum()\n",
        "\n",
        "#calculate the proportion of exceedances\n",
        "total_observations = len(data_filtered2['Return'])\n",
        "proportion_exceedances = exceedances / total_observations\n",
        "\n",
        "print(\"Proportion of Exceedances:\", proportion_exceedances)"
      ],
      "metadata": {
        "id": "DSmImlkHvsuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd879b5-3a2f-4552-cee6-6a5dc3c4bbd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proportion of Exceedances: 0.01568627450980392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B"
      ],
      "metadata": {
        "id": "pK7ayGDenrgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the expected shortfall for the VaR calculated in question A. How is the result, compared to\n",
        "the VaR?"
      ],
      "metadata": {
        "id": "tEZK-LtjxpGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter out returns worse than the VaR\n",
        "returns_worse_than_var = data_filtered2[data_filtered2['Return'] < var]['Return']\n",
        "\n",
        "#calculate the ES\n",
        "if not returns_worse_than_var.empty:\n",
        "    expected_shortfall = returns_worse_than_var.mean()\n",
        "else:\n",
        "    expected_shortfall = np.nan\n",
        "\n",
        "print(\"Expected Shortfall:\", expected_shortfall)"
      ],
      "metadata": {
        "id": "us0rpOsqkblR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a279c8-005d-4b10-b117-607e8fbb08a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected Shortfall: -0.05090042507108643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part C"
      ],
      "metadata": {
        "id": "SJxFfYim2Fz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A."
      ],
      "metadata": {
        "id": "VTT9QvmDkxrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimate the GEV parameters for the two tails of the distribution of returns, using the estimator of\n",
        "Pickands. What can you conclude about the nature of the extreme gains and losses?"
      ],
      "metadata": {
        "id": "j8FwFX6Yx2TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "V4d0YTN4N2LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pickands_estimator(data):\n",
        "\n",
        "    #fraction of data to use\n",
        "    sorted_data = np.sort(data)[::-1]\n",
        "    k = int(0.1 * len(sorted_data))\n",
        "    if k < 1:\n",
        "        raise ValueError(\"Not enough data to apply Pickands estimator.\")\n",
        "\n",
        "    #calculate the Pickands estimator for the GEV xi parameter\n",
        "    xi = (1 / np.log(2)) * np.log((sorted_data[-k] - sorted_data[-2*k]) / (sorted_data[-2*k] - sorted_data[-4*k]))\n",
        "\n",
        "    #calculate mu and sigma parameters as the mean and standard deviation of the top k data points\n",
        "    mu = np.mean(sorted_data[:k])\n",
        "    sigma = np.std(sorted_data[:k])\n",
        "\n",
        "    return {'xi': xi, 'mu': mu, 'sigma': sigma}"
      ],
      "metadata": {
        "id": "1dBAvbFX2KND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results"
      ],
      "metadata": {
        "id": "t15JSJwxN0H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#call and display of Pickands estimator\n",
        "gev_params_gains = pickands_estimator(gains)\n",
        "gev_params_losses = pickands_estimator(losses)\n",
        "\n",
        "#analyse parameters\n",
        "print(\"GEV Parameters for Gains:\", gev_params_gains)\n",
        "print(\"GEV Parameters for Losses:\", gev_params_losses)"
      ],
      "metadata": {
        "id": "nq9jtd6ECkxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99cc8d7-8f93-4e2e-dcde-45859b13fe4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEV Parameters for Gains: {'xi': -1.017904110288953, 'mu': 0.044886823821665124, 'sigma': 0.012584666884058687}\n",
            "GEV Parameters for Losses: {'xi': -0.8150241887276759, 'mu': 0.04646321776268792, 'sigma': 0.020089520576226726}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B."
      ],
      "metadata": {
        "id": "iWV_RYHqkwgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the value at risk based on EVT for various confidence levels, with the assumption of iid\n",
        "returns."
      ],
      "metadata": {
        "id": "q7JaGVAFx6EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "lge6wF5ENyBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_var_from_gev(data, confidence_level, gev_params):\n",
        "    n = len(data)\n",
        "    k = int(0.1 * n)\n",
        "\n",
        "    #fraction of data to use\n",
        "    if k < 1:\n",
        "        raise ValueError(\"Not enough data to apply Pickands estimator.\")\n",
        "    if 4*k >= n:\n",
        "        raise ValueError(\"k is too large for the dataset provided.\")\n",
        "\n",
        "    xi = gev_params['xi']\n",
        "    mu = gev_params['mu']\n",
        "    sigma = gev_params['sigma']\n",
        "\n",
        "    #calculate the necessary quantiles\n",
        "    x_n_k = data[-k]\n",
        "    x_n_2k = data[-2*k]\n",
        "\n",
        "    #calculate the VaR\n",
        "    term1 = (k / (n * (1 - confidence_level))) ** xi\n",
        "    var = x_n_k + (x_n_k * x_n_2k * ((term1 - 1)) / (1 - 2 ** (-xi)))\n",
        "\n",
        "    return var"
      ],
      "metadata": {
        "id": "PKd08r3OkwMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results"
      ],
      "metadata": {
        "id": "ZOhEI1V_NwLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#confidence levels to calculate VaR\n",
        "confidence_levels = [0.95, 0.99]\n",
        "\n",
        "# Calculate VaR for the specified confidence levels using the gains GEV parameters\n",
        "vars_gains = {conf: calculate_var_from_gev(np.sort(gains), conf, gev_params_gains) for conf in confidence_levels}\n",
        "\n",
        "# Calculate VaR for the specified confidence levels using the losses GEV parameters\n",
        "vars_losses = {conf: calculate_var_from_gev(np.sort(losses), conf, gev_params_losses) for conf in confidence_levels}\n",
        "\n",
        "print(\"Calculated VaR for gains :\", vars_gains)\n",
        "print(\"Calculated VaR for losses :\", vars_losses)"
      ],
      "metadata": {
        "id": "DCxLesEOllSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b712c91-4ed7-4795-8772-49779c3cb9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated VaR for gains : {0.95: 0.03149852778575379, 0.99: 0.03180787165738903}\n",
            "Calculated VaR for losses : {0.95: 0.03212828869324476, 0.99: 0.032507172832029814}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part D"
      ],
      "metadata": {
        "id": "lEN74AVBqpVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Estimate all the parameters of the model of Almgren and Chriss. Is this model well specified?\n"
      ],
      "metadata": {
        "id": "LMmuvZsbyFG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the framework of Almgren and Chriss, what is your liquidation strategy (we recall that you can\n",
        "only make transactions once every hour)."
      ],
      "metadata": {
        "id": "Up8ejgwTyPrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importation"
      ],
      "metadata": {
        "id": "TNJo3GMGrXAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "file_path = '/content/Dataset TD4.xlsx'\n",
        "td4_data = pd.read_excel(file_path)\n",
        "\n",
        "td4_data.columns = ['transaction_date', 'bid_ask_spread', 'volume', 'sign', 'price_before']\n",
        "\n",
        "#date to index\n",
        "td4_data['transaction_index'] = np.arange(len(td4_data))\n",
        "\n",
        "#handle NaN values\n",
        "td4_data['volume'].fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "LyboJIQLrWaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "Ml9-ozJ8NonP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function g(v)\n",
        "def g(v, gamma):\n",
        "    return gamma * v\n",
        "\n",
        "#function h(n_k/τ)\n",
        "def h(n_k, xi, eta, tau):\n",
        "    return xi * np.sign(n_k) + eta * (n_k / tau)\n",
        "\n",
        "def total_cost(df, gamma, eta, tau):\n",
        "    #handle NaN values in volume by replacing\n",
        "    df['volume'].fillna(0, inplace=True)\n",
        "\n",
        "    #price change\n",
        "    price_changes = df['price_before'].diff().fillna(0)\n",
        "\n",
        "    #v_k and n_k\n",
        "    v_k = df['volume'] / tau\n",
        "    n_k = df['volume']\n",
        "\n",
        "    #g(v) for all transactions\n",
        "    linear_costs = g(v_k, gamma)\n",
        "\n",
        "    #h(n_k/τ) for all transactions\n",
        "    quadratic_costs = h(n_k, df['bid_ask_spread'], eta, tau)\n",
        "\n",
        "    #price impact cost for all transactions\n",
        "    price_impact_costs = 0.5 * gamma * price_changes ** 2\n",
        "\n",
        "    #total cost\n",
        "    total_costs = tau * linear_costs + n_k * quadratic_costs + price_impact_costs\n",
        "\n",
        "    return total_costs.sum()"
      ],
      "metadata": {
        "id": "CCkyzLzA-jEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results"
      ],
      "metadata": {
        "id": "hh-D9P7MNsVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tau = 1\n",
        "gamma = 0.1\n",
        "eta = 0.1\n",
        "learning_rate = 0.001\n",
        "iterations = 300\n",
        "epsilon = 1e-5\n",
        "\n",
        "#initialize previous cost\n",
        "previous_cost = float('inf')\n",
        "\n",
        "#gradient descent\n",
        "for iteration in range(iterations):\n",
        "    #current cost for convergence check\n",
        "    current_cost = total_cost(td4_data, gamma, eta, tau)\n",
        "\n",
        "    #if change in cost is below tolerance, stop the optimization\n",
        "    if abs(previous_cost - current_cost) < epsilon:\n",
        "        print(f\"Convergence reached at iteration {iteration}.\")\n",
        "        break\n",
        "\n",
        "    previous_cost = current_cost\n",
        "\n",
        "    #calculation of gradients by finite difference\n",
        "    gamma_gradient = (total_cost(td4_data, gamma + epsilon, eta, tau) - total_cost(td4_data, gamma, eta, tau)) / epsilon\n",
        "    eta_gradient = (total_cost(td4_data, gamma, eta + epsilon, tau) - total_cost(td4_data, gamma, eta, tau)) / epsilon\n",
        "\n",
        "    gamma = max(0, gamma - learning_rate * gamma_gradient)\n",
        "    eta = max(0, eta - learning_rate * eta_gradient)\n",
        "\n",
        "# Print the optimized parameters\n",
        "print(\"Total Cost :\", current_cost)\n",
        "print(\"Optimized gamma:\", gamma)\n",
        "print(\"Optimized eta:\", eta)"
      ],
      "metadata": {
        "id": "a28Y0edL9TA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e8a54c-80d6-4211-99be-fef0a1229e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergence reached at iteration 2.\n",
            "Total Cost : 1505.5773\n",
            "Optimized gamma: 0\n",
            "Optimized eta: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part E"
      ],
      "metadata": {
        "id": "-0dtpzay_sP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A."
      ],
      "metadata": {
        "id": "e0X8rnPO_yIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the correlation matrix at each scale using wavelets. Determine the volatility vector with\n",
        "a scaling thanks to the Hurst exponents. From the correlation matrix and the volatility vector, calculate\n",
        "the covariance matrix and conclude about the volatility of the portfolio."
      ],
      "metadata": {
        "id": "YCNg57_ZyYlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importation"
      ],
      "metadata": {
        "id": "AB91EtSF_2g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "file_path = '/content/Dataset TD5.xlsx'\n",
        "td5_data = pd.read_excel(file_path)\n",
        "\n",
        "# Dropping unnecessary columns and rows\n",
        "td5_data = td5_data.drop(columns=['Unnamed: 3', 'Unnamed: 7'], axis=1)\n",
        "td5_data = td5_data.drop([0, 1], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Renaming the columns for clarity\n",
        "column_names = ['Date_GBPEUR', 'HIGH_GBPEUR', 'LOW_GBPEUR', 'Date_SEKEUR', 'HIGH_SEKEUR', 'LOW_SEKEUR', 'Date_CADEUR', 'HIGH_CADEUR', 'LOW_CADEUR']\n",
        "td5_data.columns = column_names\n",
        "\n",
        "#average of High and Low for each FX rate\n",
        "td5_data['AVG_GBPEUR'] = td5_data[['HIGH_GBPEUR', 'LOW_GBPEUR']].mean(axis=1)\n",
        "td5_data['AVG_SEKEUR'] = td5_data[['HIGH_SEKEUR', 'LOW_SEKEUR']].mean(axis=1)\n",
        "td5_data['AVG_CADEUR'] = td5_data[['HIGH_CADEUR', 'LOW_CADEUR']].mean(axis=1)\n",
        "\n",
        "td5_data[['Date']] = td5_data[['Date_GBPEUR']]\n",
        "\n",
        "#returns for each FX rate and set the Date as the index\n",
        "td5_data['RTN_GBPEUR'] = td5_data['AVG_GBPEUR'].pct_change()\n",
        "td5_data['RTN_SEKEUR'] = td5_data['AVG_SEKEUR'].pct_change()\n",
        "td5_data['RTN_CADEUR'] = td5_data['AVG_CADEUR'].pct_change()\n",
        "\n",
        "# Select only the average rates for analysis\n",
        "fx_rates = td5_data[['Date', 'AVG_GBPEUR', 'AVG_SEKEUR', 'AVG_CADEUR']]\n",
        "fx_rates.set_index('Date', inplace=True)\n",
        "\n",
        "# Calculating the portfolio return as the weighted average of the FX rates returns\n",
        "weights = np.array([1/3, 1/3, 1/3])  # Equal weights\n",
        "\n",
        "fx_returns = td5_data[['Date', 'RTN_GBPEUR', 'RTN_SEKEUR', 'RTN_CADEUR']]\n",
        "fx_returns.set_index('Date', inplace=True)\n",
        "\n",
        "# Calculate the portfolio return with equal weights\n",
        "fx_returns['Portfolio_Return'] = (fx_returns[['RTN_GBPEUR', 'RTN_SEKEUR', 'RTN_CADEUR']] * weights).sum(axis=1)"
      ],
      "metadata": {
        "id": "D43EijgH_43G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bd87c3-dc6e-453b-b0a6-623cd96aad35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-196-b3b480ca77b2>:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  fx_returns['Portfolio_Return'] = (fx_returns[['RTN_GBPEUR', 'RTN_SEKEUR', 'RTN_CADEUR']] * weights).sum(axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "6jLYt9NbOXYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rescaling the data\n",
        "def resample_data(dataframe, freq):\n",
        "    if freq == '1H':\n",
        "        filtered = dataframe[dataframe.index.minute == 0]\n",
        "        resampled = filtered.resample(freq).mean()\n",
        "    else:\n",
        "        resampled = dataframe.resample(freq).mean()\n",
        "\n",
        "    return resampled\n",
        "\n",
        "def wavelet_correlation(dataframe, wavelet='haar', mode='symmetric', level=1):\n",
        "    transformed_columns = []\n",
        "\n",
        "    for column in dataframe.columns:\n",
        "        coeffs = pywt.wavedec(dataframe[column].dropna(), wavelet=wavelet, mode=mode, level=level)\n",
        "        approximation = coeffs[0]\n",
        "\n",
        "        #adjust the length of the approximation\n",
        "        if len(approximation) < len(dataframe):\n",
        "            # If the approximation is shorter, extend it with NaNs\n",
        "            approximation = np.pad(approximation, (0, len(dataframe) - len(approximation)), 'constant', constant_values=np.nan)\n",
        "        elif len(approximation) > len(dataframe):\n",
        "            # If the approximation is longer, truncate it\n",
        "            approximation = approximation[:len(dataframe)]\n",
        "\n",
        "        transformed_column_name = f'WT_{column}'\n",
        "        dataframe[transformed_column_name] = approximation\n",
        "        transformed_columns.append(transformed_column_name)\n",
        "\n",
        "    #calculate the correlation matrix using the correct column names\n",
        "    correlation_matrix = dataframe[transformed_columns].corr()\n",
        "    return correlation_matrix\n",
        "\n",
        "#hurst exponent\n",
        "def hurst_exponent(ts):\n",
        "    N = len(ts)\n",
        "    M2 = np.sum([(ts[i] - ts[i - 1]) ** 2 for i in range(1, N)]) / N\n",
        "    M2_prime = np.sum([(ts[2 * i] - ts[2 * (i - 1)]) ** 2 for i in range(1, N // 2)]) / (N / 2)\n",
        "\n",
        "    if M2 == 0:\n",
        "        return np.nan\n",
        "\n",
        "    H_hat = 0.5 * np.log2(M2_prime / M2)\n",
        "    return H_hat\n",
        "\n",
        "#covariance matrix from correlation matrix and volatilities\n",
        "def calculate_covariance_matrix(correlation_matrix, volatilities):\n",
        "    volatilities_diag = np.diag(volatilities)\n",
        "\n",
        "    covariance_matrix = volatilities_diag.dot(correlation_matrix).dot(volatilities_diag)\n",
        "    return covariance_matrix\n",
        "\n",
        "\n",
        "#portfolio volatility\n",
        "def portfolio_volatility(covariance_matrix, weights):\n",
        "    return np.sqrt(np.dot(weights.T, np.dot(covariance_matrix, weights)))"
      ],
      "metadata": {
        "id": "_t4A63DjS5F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Correlation Matrix"
      ],
      "metadata": {
        "id": "8I9mazpotsrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#time scales for resampling\n",
        "scales = ['15min', '1H', '1D', '1W']\n",
        "\n",
        "correlation_matrices = {}\n",
        "\n",
        "for scale in scales:\n",
        "    #resample data\n",
        "    resampled_data = resample_data(fx_rates, scale)\n",
        "\n",
        "    #correlation matrix\n",
        "    correlation_matrix = wavelet_correlation(resampled_data)\n",
        "    correlation_matrices[scale] = correlation_matrix\n",
        "\n",
        "    print(f\"Correlation Matrix for {scale} scale:\\n{correlation_matrices[scale]}\\n\")\n"
      ],
      "metadata": {
        "id": "oI82MrOMOW1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1196ad-4bdf-4a50-c427-b97c1e163d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Matrix for 15min scale:\n",
            "               WT_AVG_GBPEUR  WT_AVG_SEKEUR  WT_AVG_CADEUR\n",
            "WT_AVG_GBPEUR       1.000000       0.814763      -0.226994\n",
            "WT_AVG_SEKEUR       0.814763       1.000000      -0.135315\n",
            "WT_AVG_CADEUR      -0.226994      -0.135315       1.000000\n",
            "\n",
            "Correlation Matrix for 1H scale:\n",
            "               WT_AVG_GBPEUR  WT_AVG_SEKEUR  WT_AVG_CADEUR\n",
            "WT_AVG_GBPEUR       1.000000       0.815376      -0.227289\n",
            "WT_AVG_SEKEUR       0.815376       1.000000      -0.136208\n",
            "WT_AVG_CADEUR      -0.227289      -0.136208       1.000000\n",
            "\n",
            "Correlation Matrix for 1D scale:\n",
            "               WT_AVG_GBPEUR  WT_AVG_SEKEUR  WT_AVG_CADEUR\n",
            "WT_AVG_GBPEUR       1.000000       0.822674      -0.256434\n",
            "WT_AVG_SEKEUR       0.822674       1.000000      -0.178265\n",
            "WT_AVG_CADEUR      -0.256434      -0.178265       1.000000\n",
            "\n",
            "Correlation Matrix for 1W scale:\n",
            "               WT_AVG_GBPEUR  WT_AVG_SEKEUR  WT_AVG_CADEUR\n",
            "WT_AVG_GBPEUR       1.000000       0.859594      -0.347460\n",
            "WT_AVG_SEKEUR       0.859594       1.000000      -0.307542\n",
            "WT_AVG_CADEUR      -0.347460      -0.307542       1.000000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Volatility with Hurst exposent"
      ],
      "metadata": {
        "id": "Qeq54-zOtwo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hurst exponent\n",
        "hurst_exponents = fx_rates.apply(hurst_exponent)\n",
        "\n",
        "#daily volatility\n",
        "daily_volatilities = fx_rates.diff().apply(np.std)\n",
        "\n",
        "#volatilities by the Hurst exponent\n",
        "scaled_volatilities = daily_volatilities * hurst_exponents\n",
        "\n",
        "#results\n",
        "print(\"Hurst Exponents:\\n\", hurst_exponents)\n",
        "print(\"Daily Volatilities:\\n\", daily_volatilities)\n",
        "print(\"Scaled Volatilities:\\n\", scaled_volatilities)"
      ],
      "metadata": {
        "id": "9eWJTrhDTXsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c3937f-126c-4eb3-a26a-03ca44409a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hurst Exponents:\n",
            " AVG_GBPEUR    0.671373\n",
            "AVG_SEKEUR    0.654591\n",
            "AVG_CADEUR    0.655240\n",
            "dtype: float64\n",
            "Daily Volatilities:\n",
            " AVG_GBPEUR    0.000775\n",
            "AVG_SEKEUR    0.000035\n",
            "AVG_CADEUR    0.000348\n",
            "dtype: float64\n",
            "Scaled Volatilities:\n",
            " AVG_GBPEUR    0.000521\n",
            "AVG_SEKEUR    0.000023\n",
            "AVG_CADEUR    0.000228\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Covariance Matrix"
      ],
      "metadata": {
        "id": "Mt-gValUt6aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#covariance matrices\n",
        "covariance_matrices = {}\n",
        "for scale in scales:\n",
        "    #correlation matrix\n",
        "    correlation_matrix = correlation_matrices[scale]\n",
        "\n",
        "    covariance_matrix = calculate_covariance_matrix(correlation_matrix, scaled_volatilities)\n",
        "    covariance_matrices[scale] = covariance_matrix\n",
        "\n",
        "#weights of each assets\n",
        "weights = np.array([1/3, 1/3, 1/3])\n",
        "\n",
        "#portfolio volatility for each scale\n",
        "portfolio_volatilities = {}\n",
        "for scale, covariance_matrix in covariance_matrices.items():\n",
        "    vol = portfolio_volatility(covariance_matrix, weights)\n",
        "    portfolio_volatilities[scale] = vol\n",
        "\n",
        "#portfolio volatilities\n",
        "for scale, vol in portfolio_volatilities.items():\n",
        "    print(f\"Portfolio Volatility for {scale} scale: {vol}\")"
      ],
      "metadata": {
        "id": "J5OTVld4tMSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a6e077-1c75-40fd-e3cc-05296e4df031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Portfolio Volatility for 15min scale: 0.00017875819532757874\n",
            "Portfolio Volatility for 1H scale: 0.00017873807694286864\n",
            "Portfolio Volatility for 1D scale: 0.00017649201277009075\n",
            "Portfolio Volatility for 1W scale: 0.00016940297348471312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B."
      ],
      "metadata": {
        "id": "Y1Y4xOb4JmgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the volatility directly from the series of prices of the portfolio and justify your\n",
        "methodological choices (for example with overlapping returns or not)."
      ],
      "metadata": {
        "id": "IOV7nmlqycke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#time scales\n",
        "time_scales = [(15,60,1440,10080),(\"15min\",\"1H\",\"1D\",\"1W\")]\n",
        "i=0\n",
        "volatility_dict = {}\n",
        "\n",
        "for scale in time_scales[0]:\n",
        "    #resample the portfolio returns data for each time scale and calculate the mean\n",
        "    resampled_returns = fx_returns['Portfolio_Return'].resample(f'{scale}T').mean()\n",
        "\n",
        "    #standard deviation (volatility) for each time scale\n",
        "    volatility_corrected = resampled_returns.std()\n",
        "    volatility_dict[scale] = volatility_corrected\n",
        "\n",
        "#portfolio volatilities\n",
        "for scale, vol in volatility_dict.items():\n",
        "    print(\"Portfolio Volatility for \", time_scales[1][i], f\"scale: {vol}\")\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "qlklhUPMJoC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e39c7e-bac5-4a93-f8b0-df0184fc3c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Portfolio Volatility for  15min scale: 0.00034131077611115635\n",
            "Portfolio Volatility for  1H scale: 0.0001965199615230007\n",
            "Portfolio Volatility for  1D scale: 6.453290289445081e-05\n",
            "Portfolio Volatility for  1W scale: 1.7611217930234627e-05\n"
          ]
        }
      ]
    }
  ]
}